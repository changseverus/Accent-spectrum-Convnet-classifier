{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "import keras\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "epochs=20\n",
    "num_classes=3\n",
    "\n",
    "# Load the npy file saved from processing data\n",
    "# Onehotencode the labeled csv file, and transform into numpy array\n",
    "\n",
    "train_data=np.load('train_data.npy')\n",
    "df=pd.read_csv(\"train_labels.csv\")\n",
    "y_train=onehotencoder.fit_transform(df).toarray()\n",
    "y_train=np.delete(y_train,3,axis=1)\n",
    "train_label=y_train[np.arange(6738)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test-set image and label \n",
    "\n",
    "test_data=np.load(test_data.npy)\n",
    "df1=pd.read_csv(\"test_labels.csv\")\n",
    "y_test=onehotencoder.fit_transform(df1).toarray()\n",
    "y_test=np.delete(y_test,3,axis=1)\n",
    "test_label=y_test[np.arange(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 126, 171, 24)      888       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 126, 171, 24)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 63, 85, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 61, 83, 48)        10416     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 61, 83, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 61, 83, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 59, 81, 96)        41568     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 59, 81, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 111360)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 111360)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 334083    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 386,955\n",
      "Trainable params: 386,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# Define a CNN classifier network composed of a sequence of layers\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer accepts an image and applies a convolution that uses 24 3x3 filters and a rectified linear unit activation function\n",
    "# Next add a max pooling layer with a 2*2 patch\n",
    "\n",
    "model.add(Conv2D(24, (3,3), input_shape=(128,173,4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add more layers, including dropout layer that prevent over-fitting\n",
    "model.add(Conv2D(48, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(96, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Now we'll flatten the feature maps and generate an output layer with a predicted probability for each class\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# With the layers defined, we can now compile the model for categorical (multi-class) classification\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6738 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "6738/6738 [==============================] - 346s 51ms/step - loss: 1.2002 - acc: 0.4061 - val_loss: 1.0525 - val_acc: 0.4520\n",
      "Epoch 2/20\n",
      "6738/6738 [==============================] - 372s 55ms/step - loss: 0.9700 - acc: 0.5255 - val_loss: 0.9680 - val_acc: 0.5063\n",
      "Epoch 3/20\n",
      "6738/6738 [==============================] - 379s 56ms/step - loss: 0.8676 - acc: 0.6060 - val_loss: 0.8883 - val_acc: 0.5813\n",
      "Epoch 4/20\n",
      "6738/6738 [==============================] - 382s 57ms/step - loss: 0.7989 - acc: 0.6457 - val_loss: 0.8200 - val_acc: 0.6287\n",
      "Epoch 5/20\n",
      "6738/6738 [==============================] - 382s 57ms/step - loss: 0.7176 - acc: 0.6949 - val_loss: 0.7470 - val_acc: 0.6727\n",
      "Epoch 6/20\n",
      "6738/6738 [==============================] - 384s 57ms/step - loss: 0.6306 - acc: 0.7462 - val_loss: 0.6777 - val_acc: 0.7147\n",
      "Epoch 7/20\n",
      "6738/6738 [==============================] - 428s 63ms/step - loss: 0.5725 - acc: 0.7707 - val_loss: 0.5859 - val_acc: 0.7637\n",
      "Epoch 8/20\n",
      "6738/6738 [==============================] - 440s 65ms/step - loss: 0.5135 - acc: 0.7968 - val_loss: 0.5109 - val_acc: 0.8077\n",
      "Epoch 9/20\n",
      "6738/6738 [==============================] - 479s 71ms/step - loss: 0.4659 - acc: 0.8134 - val_loss: 0.4731 - val_acc: 0.8320\n",
      "Epoch 10/20\n",
      "6738/6738 [==============================] - 480s 71ms/step - loss: 0.4175 - acc: 0.8375 - val_loss: 0.4011 - val_acc: 0.8520\n",
      "Epoch 11/20\n",
      "6738/6738 [==============================] - 480s 71ms/step - loss: 0.3786 - acc: 0.8554 - val_loss: 0.3070 - val_acc: 0.9073\n",
      "Epoch 12/20\n",
      "6738/6738 [==============================] - 491s 73ms/step - loss: 0.3196 - acc: 0.8749 - val_loss: 0.2602 - val_acc: 0.9207\n",
      "Epoch 13/20\n",
      "6738/6738 [==============================] - 515s 76ms/step - loss: 0.2857 - acc: 0.8888 - val_loss: 0.2482 - val_acc: 0.9267\n",
      "Epoch 14/20\n",
      "6738/6738 [==============================] - 527s 78ms/step - loss: 0.2516 - acc: 0.9032 - val_loss: 0.1710 - val_acc: 0.9580\n",
      "Epoch 15/20\n",
      "6738/6738 [==============================] - 529s 78ms/step - loss: 0.2307 - acc: 0.9138 - val_loss: 0.1779 - val_acc: 0.9563\n",
      "Epoch 16/20\n",
      "6738/6738 [==============================] - 541s 80ms/step - loss: 0.2031 - acc: 0.9230 - val_loss: 0.1087 - val_acc: 0.9773\n",
      "Epoch 17/20\n",
      "6738/6738 [==============================] - 541s 80ms/step - loss: 0.1760 - acc: 0.9316 - val_loss: 0.0956 - val_acc: 0.9790\n",
      "Epoch 18/20\n",
      "6738/6738 [==============================] - 564s 84ms/step - loss: 0.1596 - acc: 0.9417 - val_loss: 0.0614 - val_acc: 0.9883\n",
      "Epoch 19/20\n",
      "6738/6738 [==============================] - 566s 84ms/step - loss: 0.1508 - acc: 0.9427 - val_loss: 0.0619 - val_acc: 0.9903\n",
      "Epoch 20/20\n",
      "6738/6738 [==============================] - 568s 84ms/step - loss: 0.1312 - acc: 0.9513 - val_loss: 0.0470 - val_acc: 0.9943\n",
      "Test loss: 0.04697986044486364\n",
      "Test accuracy: 0.9943333333333333\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_data, test_label))\n",
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save('optimized_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

